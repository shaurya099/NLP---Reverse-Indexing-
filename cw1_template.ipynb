{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "class InvertedIndex:\n",
    "    def __init__(self):\n",
    "        self.textDoc = []\n",
    "        self.wordTokens = []\n",
    "        self.invertedIndex = {}\n",
    "        self.episodeList = [3.1,3.2,3.3,3.4,3.5,3.6,3.7,3.8,3.9,\"3.10\",3.11,3.12,3.13,3.14,3.15,3.16,3.17,3.18,3.19,\"3.20\",3.21,3.22,3.23,3.24,4.1,4.2,4.3,4.4,4.5,4.6,4.7,4.8,4.9,\"4.10\",4.11,4.12,4.13,4.14,4.15,4.16,4.17,4.18,4.19,\"4.20\",4.21,4.22]\n",
    "        self.corpusAnalysis = {}\n",
    "    def read_data(self, path: str):\n",
    "        \"\"\"\n",
    "        Read files from a directory and then append the data of each file into a list.\n",
    "        \"\"\"\n",
    "        with open(path) as infile:\n",
    "            self.textDoc= infile.read()\n",
    "        infile.close()\n",
    "        \n",
    "    def unwantedWords(self):\n",
    "        listOfUnwantedWords = [\"show\",\"season\",\"reference\",\"references\",\"scene\",\"television\", \"rating\",\"production\", \"written\",\"nielsen\",\"reception\",\"wikipedia\", \"plotedit\",\"wikiedia\", \"receptionedit\",\"referencesedit\",\"jump\",\"navigation\",\"search\"]\n",
    "        tempList = []\n",
    "        for w in self.wordTokens:\n",
    "            if w not in listOfUnwantedWords:\n",
    "                tempList.append(w)\n",
    "        self.wordTokens = tempList\n",
    "        \n",
    "    def remove_References(self):\n",
    "        removingReferencesPattern = r'[\\[\\d\\]]'\n",
    "        self.textDoc = re.sub(removingReferencesPattern,\"\", self.textDoc)\n",
    "        \n",
    "    \n",
    "    def remove_Edit_Blocks(self):\n",
    "        removeEditBlocksPattern = r'\\[edit\\]'\n",
    "        self.TextDoc = re.sub(removeEditBlocksPattern,\"\", self.textDoc)\n",
    "       \n",
    "    \n",
    "    def remove_Non_Alpha_Numeric_Characters(self):\n",
    "        self.textDoc = re.sub(\"[^\\w]\", \" \",  self.textDoc)\n",
    "        \n",
    "    \n",
    "    def case_Folding(self):\n",
    "        self.textDoc = self.textDoc.lower()\n",
    "    \n",
    "    def word_Tokenisation(self):\n",
    "        self.wordTokens = word_tokenize(self.textDoc)\n",
    "        #print(\"tokenizationDone\")\n",
    "    \n",
    "    def stop_words_Removal(self):\n",
    "        tempList = []\n",
    "        stopWords = set(stopwords.words(\"english\"))\n",
    "        #print(\"set\")\n",
    "        for w in self.wordTokens:\n",
    "            if w not in stopWords: \n",
    "                tempList.append(w)\n",
    "        self.wordTokens = tempList\n",
    "        #print(\"stopword removal done\")\n",
    "        \n",
    "    def tokenStemming(self):\n",
    "        stemmer = PorterStemmer()\n",
    "        tempList = []\n",
    "        for tokens in self.wordTokens:\n",
    "            tempList.append(stemmer.stem(tokens))\n",
    "        self.workTokens = tempList\n",
    "        #print(\"stemming done\")\n",
    "    \n",
    "    def tokenLemmatization(self):\n",
    "        tempList = []\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        for tokens in self.wordTokens:\n",
    "            tempList.append(lemmatizer.lemmatize(tokens))\n",
    "        self.wordTokens = tempList\n",
    "        #print(\"lemmatization done\")\n",
    "    def process_document(self):\n",
    "        \"\"\"\n",
    "        pre-process a document and return a list of its terms\n",
    "        str->list\"\"\"\n",
    "        self.remove_References()\n",
    "        self.remove_Edit_Blocks()\n",
    "        self.remove_Non_Alpha_Numeric_Characters()\n",
    "        self.case_Folding()\n",
    "        #print(\"casefolding done\")\n",
    "        self.word_Tokenisation()\n",
    "        self.stop_words_Removal()\n",
    "        self.unwantedWords()\n",
    "        self.tokenStemming()\n",
    "        self.tokenLemmatization()\n",
    "        #return(self.wordTokens)\n",
    "    '''\n",
    "    def generate_Inverted_Index_With_Word_Count(self):\n",
    "        for w in self.wordTokens:\n",
    "            if w not in self.invertedIndex.keys():\n",
    "                self.invertedIndex[w] =[]\n",
    "            \n",
    "            for position, item in enumerate(self.wordTokens):\n",
    "                episodeString = \"{} : {}\"\n",
    "                if item in iself.invertedIndex.keys():\n",
    "                    self.invertedIndex[item].append(episodeString.format(episode,position))\n",
    "                else:\n",
    "                    self.invertedIndex[item] =[]\t\n",
    "                    self.invertedIndex[item].append(episodeString.format(episode,position))\n",
    "        self.wordTokens = []\n",
    "    '''            \n",
    "    \n",
    "    def index_corpus(self, episode):\n",
    "        \"\"\"\n",
    "        index given documents\n",
    "        list->None\"\"\"\n",
    "    \n",
    "        for position, item in enumerate(self.wordTokens):\n",
    "            episodeString = \"{} : {}\"\n",
    "            if item in self.invertedIndex.keys():\n",
    "                self.invertedIndex[item].append(episodeString.format(episode,position))\n",
    "            else:\n",
    "                self.invertedIndex[item] =[]\t\n",
    "                self.invertedIndex[item].append(episodeString.format(episode,position))\n",
    "        self.wordTokens = []\n",
    "        \n",
    "     \n",
    "    def proximity_search(self, term1: str, term2: str) -> set:\n",
    "        \"\"\"\n",
    "        1) check whether given two terms appear within a window\n",
    "        2) calculate the number of their co-existance in a document\n",
    "        3) add the document id and the number of matches into a dict\n",
    "        return the dict\"\"\"\n",
    "        word1 = self.invertedIndex[term1]\n",
    "        word2 = self.invertedIndex[term2]\n",
    "        matchedEpisodes = set()\n",
    "        for i in range(len(word1)): \n",
    "            for j in range(len(word2)):\n",
    "                proximityDifference = 0\n",
    "                Word1 = word1[i].split(\":\")\n",
    "                Word2 = word2[j].split(\":\")\n",
    "                if Word1[0] == Word2[0]:\n",
    "                    proximityDifference = int(Word1[1]) - int(Word2[1])\n",
    "                if abs(proximityDifference) <= 3:\n",
    "                    matchedEpisodes.add(Word1[0])\n",
    "        return matchedEpisodes            \n",
    "    \n",
    "    def createCorpusAnalysis(self, episode):\n",
    "        for w in self.wordTokens: \n",
    "            if w not in self.corpusAnalysis.keys():\n",
    "                self.corpusAnalysis[w] = 1\t\n",
    "            else:\n",
    "                self.corpusAnalysis[w] += 1 \n",
    "        self.wordTokens = []\n",
    "        \n",
    "        \n",
    "    def displayTopOccurences(self): \n",
    "        topTerms= {}\n",
    "        for x in list(reversed(list(self.corpusAnalysis)))[0:99]:\n",
    "            topTerms[x] = self.corpusAnalysis[x]\n",
    "        print(topTerms)\n",
    "        \n",
    "    def sortCorpusAnalysis(self, episode):\n",
    "        sortedDict = {}\n",
    "        sortedKeys = sorted(self.corpusAnalysis, key=self.corpusAnalysis.get)\n",
    "        for w in sortedKeys:\n",
    "            sortedDict[w] = self.corpusAnalysis[w]\n",
    "        self.corpusAnalysis = sortedDict  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'episode': 1530, 'homer': 837, 'simpson': 612, 'bart': 517, 'lisa': 388, 'marge': 310, 'one': 262, 'film': 207, 'first': 172, 'guest': 171, 'song': 168, 'krusty': 161, 'fox': 157, 'writer': 147, 'burn': 146, 'family': 145, 'jean': 141, 'series': 136, 'original': 129, 'time': 129, 'best': 128, 'get': 128, 'mr': 124, 'make': 123, 'directed': 123, 'springfield': 123, 'appearance': 121, 'week': 120, 'also': 120, 'said': 118, 'al': 117, 'character': 116, 'american': 111, 'voice': 107, 'gag': 107, 'named': 107, 'new': 106, 'groening': 106, 'aired': 105, 'parody': 103, 'plot': 103, 'day': 101, 'cultural': 101, 'reiss': 99, 'star': 98, 'like': 97, 'producer': 97, 'two': 95, 'list': 95, 'originally': 94, 'would': 94, 'mike': 91, 'network': 90, 'feature': 89, 'couch': 87, 'matt': 84, 'later': 83, 'book': 80, 'go': 80, 'tell': 79, 'school': 78, 'received': 78, 'moe': 78, 'next': 78, 'called': 77, 'rated': 75, 'th': 75, 'previous': 74, 'air': 74, 'play': 73, 'home': 73, 'state': 73, 'dvd': 73, 'david': 72, 'sequence': 71, 'name': 71, 'guide': 70, 'made': 70, 'several': 70, 'jackson': 69, 'end': 69, 'based': 69, 'love': 68, 'line': 68, 'story': 67, 'animated': 67, 'many': 67, 'maggie': 66, 'second': 66, 'child': 66, 'martin': 64, 'movie': 64, 'united': 64, 'third': 63, 'free': 63, 'favorite': 63, 'john': 63, 'selma': 62, 'well': 62}\n"
     ]
    }
   ],
   "source": [
    "def remove_Non_Alpha_Numeric_Characters(query):\n",
    "    query = re.sub(\"[^\\w]\", \" \",  query)\n",
    "    return query\n",
    "    \n",
    "def case_Folding(query):\n",
    "    query = query.lower()\n",
    "    return query\n",
    "        \n",
    "def stop_words_Removal(wordTokens):\n",
    "    tempList = []\n",
    "    stopWords = set(stopwords.words(\"english\"))\n",
    "    for w in wordTokens:\n",
    "        if w not in stopWords: \n",
    "            tempList.append(w)\n",
    "    wordTokens = tempList       \n",
    "    return wordTokens\n",
    "\n",
    "def tokenStemming(wordTokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    tempList = []\n",
    "    for tokens in wordTokens:\n",
    "        tempList.append(stemmer.stem(tokens))\n",
    "    workTokens = tempList        \n",
    "    return wordTokens\n",
    "\n",
    "def tokenLemmatization(wordTokens):\n",
    "    tempList = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for tokens in wordTokens:\n",
    "        tempList.append(lemmatizer.lemmatize(tokens))\n",
    "    wordTokens = tempList\n",
    "    return wordTokens\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    \"main call function\"\n",
    "    index = InvertedIndex()# initilaise the index\n",
    "    for episode in index.episodeList:\n",
    "        pathString = \"/Users/shaurya/coding-projects/NLP---Reverse-Indexing-/Simpsons/{}.txt\"\n",
    "        corpus = index.read_data(pathString.format(episode))\n",
    "        index.process_document()\n",
    "        index.index_corpus(episode) # index documents/corpus\n",
    "    \n",
    "    for episode in index.episodeList:\n",
    "        pathString = \"/Users/shaurya/coding-projects/NLP---Reverse-Indexing-/Simpsons/{}.txt\"\n",
    "        corpus = index.read_data(pathString.format(episode))\n",
    "        index.process_document()\n",
    "        index.createCorpusAnalysis(episode)# index documents/corpus\n",
    "        index.sortCorpusAnalysis(episode)\n",
    "    \n",
    "    index.displayTopOccurences()\n",
    "    \n",
    "    \n",
    "    search_term = input(\"Enter your query: \")\n",
    "    if search_term == \"corpus analysis\":\n",
    "        print(index.corpusAnalysis)\n",
    "    original_search_term = search_term\n",
    "    search_term = remove_Non_Alpha_Numeric_Characters(search_term)\n",
    "    search_term = case_Folding(search_term)                                                   \n",
    "    list_search_terms = search_term.split(\" \")\n",
    "    list_search_terms = stop_words_Removal(list_search_terms)\n",
    "    list_search_terms = tokenStemming(list_search_terms)\n",
    "    list_search_terms = tokenLemmatization(list_search_terms)\n",
    "\n",
    "    if len(list_search_terms) == 1:\n",
    "        if(list_search_terms[0]) in index.invertedIndex.keys():\n",
    "            print(\"the query is best found in episode: \")\n",
    "            print(index.invertedIndex[search_term])\n",
    "        else:\n",
    "            print(\"Search term not found\")\n",
    "            \n",
    "    elif len(list_search_terms) == 2:\n",
    "        if(list_search_terms[0] in index.invertedIndex.keys() and list_search_terms[1] in index.invertedIndex.keys()):\n",
    "            appearances = index.proximity_search(list_search_terms[0],list_search_terms[1])\n",
    "            number_of_appearances = len(index.proximity_search(list_search_terms[0],list_search_terms[1]))\n",
    "            message = \"{} appears in {} episodes\"\n",
    "            print(message.format(original_search_term,number_of_appearances))\n",
    "            print(appearances)\n",
    "        else:\n",
    "            print(\"Search term not found\")\n",
    "    \n",
    "    else: \n",
    "        print(\"Please limit your query to one or two terms\")\n",
    "    \n",
    "    print(len(index.invertedIndex.values()))\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
